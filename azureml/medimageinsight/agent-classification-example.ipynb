{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d3f4f57",
   "metadata": {},
   "source": [
    "# Building an AI Agent for Medical Image Classification with MedImageInsight\n",
    "\n",
    "This tutorial demonstrates how to build an intelligent conversational agent that can classify medical images using MedImageInsight embeddings. The agent uses Semantic Kernel to coordinate natural language interactions with image classification capabilities, allowing users to ask questions about medical images in plain language.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before proceeding with this tutorial, you need to perform some initial setup.\n",
    "\n",
    "### Required Models\n",
    "\n",
    "This notebook requires the following:\n",
    "- **MedImageInsight** (MI2)\n",
    "- **Azure OpenAI** (GPT-4o, GPT-4.1 or GPT-5)\n",
    "\n",
    "### Setup\n",
    "\n",
    "Follow the [Getting Started](../../README.md#getting-started) instructions in the main README to:\n",
    "1. Deploy the required models\n",
    "2. Set up your environment with the `.env` file\n",
    "3. Install the Healthcare AI Toolkit\n",
    "4. Download sample data\n",
    "\n",
    "Additionally, install Semantic Kernel:\n",
    "```bash\n",
    "pip install semantic-kernel\n",
    "```\n",
    "\n",
    "## Architecture\n",
    "\n",
    "The agent architecture consists of several key components:\n",
    "\n",
    "1. **DataAccess**: Manages storage and retrieval of images by unique IDs\n",
    "2. **ChatContext**: Coordinates conversation flow and data management\n",
    "3. **ImageClassificationPlugin**: Integrates MedImageInsight for classification\n",
    "4. **ChatCompletionAgent**: Orchestrates LLM-based natural language understanding\n",
    "\n",
    "The workflow is as follows:\n",
    "- User sends a message with an attached image\n",
    "- Image is stored and assigned a unique ID\n",
    "- Agent processes the natural language query\n",
    "- Agent calls the classification plugin with extracted labels\n",
    "- Plugin computes embeddings and similarity scores\n",
    "- Results are returned to the user in natural language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6de260c",
   "metadata": {},
   "source": [
    "## 1. Setup and Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a99bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "from semantic_kernel.functions import kernel_function\n",
    "from semantic_kernel.agents import ChatCompletionAgent\n",
    "from typing import Annotated\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d06dacf",
   "metadata": {},
   "source": [
    "## 2. Chat Context Management and Data Access Layer\n",
    "\n",
    "The `ChatContext` and `DataAccess` classes form the architectural foundation of our agent system. Together, they solve a fundamental challenge: **how to coordinate stateful conversations with binary image data when agents can only pass text between function calls**.\n",
    "\n",
    "### The Architecture Challenge\n",
    "\n",
    "When building conversational agents with Semantic Kernel, all function parameters must be serializable to JSON for the language model to process. This creates a problem for medical imaging applications:\n",
    "- Images are binary data (bytes) that cannot be directly passed through text-based interfaces\n",
    "- Multiple function calls need access to the same image data\n",
    "- Expensive resources (like the MI2 client) should be initialized once and shared across all interactions\n",
    "\n",
    "**Important:** This is **not** a typical multi-modal approach where images are sent directly to the LLM for vision-based understanding. Instead, images are routed to the MedImageInsight classification tool. The LLM only processes text—it understands the user's intent and extracts classification labels, then calls the appropriate function with an image ID reference.\n",
    "\n",
    "### The Solution: Context + Data Access Pattern\n",
    "\n",
    "We separate concerns into two complementary classes:\n",
    "\n",
    "**`DataAccess`** - Handles the storage/retrieval problem:\n",
    "1. Stores image bytes in memory (or database/blob storage in production)\n",
    "2. Generates unique text identifiers (UUIDs) for each image\n",
    "3. Allows functions to retrieve images using these text IDs\n",
    "4. Provides state management (list, store, retrieve operations)\n",
    "\n",
    "**`ChatContext`** - Handles the orchestration problem:\n",
    "1. Coordinates the three critical components: DataAccess, MI2 Client, and Agent\n",
    "2. Maintains shared state across the entire conversation\n",
    "3. Provides a high-level interface (`send_message`) that abstracts complexity\n",
    "4. Handles image format conversion (file paths, PIL Images, bytes) automatically\n",
    "\n",
    "### How They Work Together\n",
    "\n",
    "The workflow follows these steps:\n",
    "\n",
    "1. **User sends image** → `ChatContext.send_message()` receives the image and message\n",
    "2. **Image conversion** → ChatContext converts the image to bytes (from file path, PIL Image, etc.)\n",
    "3. **Image storage** → `DataAccess.store_image()` stores the bytes and generates a unique ID (e.g., \"abc123\")\n",
    "4. **Message preparation** → ChatContext appends the image ID to the user's message: `\"Is this pneumonia?\\nImage ID: abc123\"`\n",
    "5. **Agent processing** → The Agent (LLM) receives only text—no binary data. It understands the question and extracts labels\n",
    "6. **Function call** → Agent invokes `classify_image(labels=[\"pneumonia\", \"normal\"], image_id=\"abc123\")`\n",
    "7. **Image retrieval** → Plugin calls `DataAccess.get_image(\"abc123\")` to retrieve the actual image bytes\n",
    "8. **Classification** → Plugin sends bytes to MedImageInsight for embedding and similarity calculation\n",
    "9. **Results** → Probabilities are returned to the agent, which formats them for the user\n",
    "\n",
    "**Key point:** The image travels through a separate path (ChatContext → DataAccess → Plugin → MI2) while the LLM only sees text references (image IDs) and orchestrates the workflow.\n",
    "\n",
    "### Production Considerations\n",
    "\n",
    "In production systems, replace the in-memory `DataAccess._images` dictionary with:\n",
    "- Azure Blob Storage for large-scale deployments\n",
    "- Redis or another caching layer for distributed systems\n",
    "- Database storage with proper indexing for audit trails\n",
    "\n",
    "The architectural pattern remains the same—only the storage backend changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64c7fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataAccess:\n",
    "    \"\"\"Stores and retrieves images by ID.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._images = {}\n",
    "\n",
    "    def store_image(self, image: bytes) -> str:\n",
    "        \"\"\"Store an image and return its ID.\"\"\"\n",
    "        image_id = str(uuid.uuid4())[:6]\n",
    "        self._images[image_id] = image\n",
    "        return image_id\n",
    "\n",
    "    def get_image(self, image_id: str) -> bytes:\n",
    "        \"\"\"Retrieve an image by ID.\"\"\"\n",
    "        if image_id in self._images:\n",
    "            return self._images.get(image_id)\n",
    "        print(f\"Image ID {image_id} not found!\")\n",
    "\n",
    "    def list_images(self):\n",
    "        \"\"\"List all stored image IDs.\"\"\"\n",
    "        return list(self._images.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e59edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from healthcareai_toolkit.clients import MedImageInsightClient\n",
    "from typing import Union\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "import io\n",
    "\n",
    "\n",
    "def print_message(header, message):\n",
    "    agent_header = f\" {header.strip()} \"\n",
    "    print(f\"{agent_header:-^50}\")\n",
    "    print(message)\n",
    "    print(\"-\" * 50)\n",
    "    print()\n",
    "\n",
    "\n",
    "class ChatContext:\n",
    "    \"\"\"Manages conversation context and data access.\"\"\"\n",
    "\n",
    "    def __init__(self, data_access=None, mi2_client=None):\n",
    "        self.agent = None\n",
    "        self.data_access = data_access or DataAccess()\n",
    "        self.mi2_client = mi2_client or MedImageInsightClient()\n",
    "\n",
    "    def set_agent(self, agent: ChatCompletionAgent):\n",
    "        \"\"\"Set the agent after initialization.\"\"\"\n",
    "        self.agent = agent\n",
    "\n",
    "    async def send_message(\n",
    "        self, message: str, image: Union[bytes, str, Path, Image.Image] = None\n",
    "    ):\n",
    "        \"\"\"Send a message with optional image to the agent.\n",
    "\n",
    "        Args:\n",
    "            message: The text message to send\n",
    "            image: Can be bytes, file path (str/Path), or PIL Image\n",
    "        \"\"\"\n",
    "\n",
    "        if self.agent is None:\n",
    "            raise ValueError(\"Agent not set. Call set_agent() first.\")\n",
    "\n",
    "        if image is not None:\n",
    "            # Convert image to bytes\n",
    "            if isinstance(image, (str, Path)):\n",
    "                with open(image, \"rb\") as f:\n",
    "                    image_bytes = f.read()\n",
    "            elif isinstance(image, Image.Image):\n",
    "                buffer = io.BytesIO()\n",
    "                image.save(buffer, format=image.format or \"PNG\")\n",
    "                image_bytes = buffer.getvalue()\n",
    "            elif isinstance(image, bytes):\n",
    "                image_bytes = image\n",
    "            else:\n",
    "                raise TypeError(f\"Unsupported image type: {type(image)}\")\n",
    "\n",
    "            display_image = Image.open(io.BytesIO(image_bytes))\n",
    "            display(display_image)\n",
    "\n",
    "            # Store image and get ID\n",
    "            image_id = self.data_access.store_image(image_bytes)\n",
    "            full_message = f\"{message}\\nImage ID: {image_id}\"\n",
    "        else:\n",
    "            full_message = message\n",
    "\n",
    "        # Print user message\n",
    "        print_message(\"User\", full_message)\n",
    "\n",
    "        # Send to agent\n",
    "        agent_header = f\" Agent ({self.agent.name}) \"\n",
    "        async for response in self.agent.invoke(full_message):\n",
    "            print_message(agent_header, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22a9f38",
   "metadata": {},
   "source": [
    "## 4. Initialize Semantic Kernel\n",
    "\n",
    "Here we initialize the Semantic Kernel and configure it with Azure OpenAI services. The kernel acts as the central orchestrator for our agent, managing:\n",
    "- AI service connections\n",
    "- Plugin registration\n",
    "- Function execution\n",
    "\n",
    "We load configuration from the environment settings managed by the healthcare AI toolkit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfa4f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from healthcareai_toolkit import settings\n",
    "\n",
    "# Initialize kernel\n",
    "kernel = Kernel()\n",
    "\n",
    "# Add Azure OpenAI service\n",
    "kernel.add_service(\n",
    "    AzureChatCompletion(\n",
    "        endpoint=settings.AZURE_OPENAI_ENDPOINT,\n",
    "        api_key=settings.AZURE_OPENAI_API_KEY,\n",
    "        deployment_name=settings.AZURE_OPENAI_DEPLOYMENT_NAME,\n",
    "        api_version=settings.AZURE_OPENAI_API_VERSION,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14c98ea",
   "metadata": {},
   "source": [
    "## 5. Image Classification Plugin\n",
    "\n",
    "The `ImageClassificationPlugin` is the core component that integrates MedImageInsight with the agent framework. It performs zero-shot classification using the same embedding-based approach described in the [zero-shot classification tutorial](./zero-shot-classification.ipynb).\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. **Embedding Generation**:\n",
    "   - `_get_image_embeddings()`: Converts image bytes to feature vectors using MedImageInsight\n",
    "   - `_get_text_embeddings()`: Converts text labels to feature vectors\n",
    "\n",
    "2. **Similarity Calculation**:\n",
    "   - Computes dot product between image and text embeddings\n",
    "   - Applies learned temperature scaling factor\n",
    "   - Uses softmax to convert logits to probabilities\n",
    "\n",
    "3. **Agent Integration**:\n",
    "   - The `@kernel_function` decorator exposes `classify_image()` to the agent\n",
    "   - Type annotations provide clear parameter documentation for the LLM\n",
    "   - Returns a dictionary mapping categories to probability scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88a6ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ImageClassificationPlugin:\n",
    "    \"\"\"Plugin that classifies images.\"\"\"\n",
    "\n",
    "    def __init__(self, chat_context: ChatContext):\n",
    "        self.chat_context = chat_context\n",
    "        self.data_access = chat_context.data_access\n",
    "        self.mi2_client = chat_context.mi2_client\n",
    "        self.scaling_factor = np.atleast_1d(\n",
    "            self.mi2_client.submit(text_list=[\"placeholder\"])[0][\"scaling_factor\"]\n",
    "        )\n",
    "\n",
    "    def _get_image_embeddings(self, image_data: bytes):\n",
    "        \"\"\"Get image embeddings using MedImageInsightClient.\"\"\"\n",
    "        response = self.mi2_client.submit(image_list=[image_data])\n",
    "        return np.array(response[0][\"image_features\"][0])\n",
    "\n",
    "    def _get_text_embeddings(self, texts: List[str]):\n",
    "        \"\"\"Get text embeddings using MedImageInsightClient.\"\"\"\n",
    "        response = self.mi2_client.submit(text_list=texts)\n",
    "        return np.array([item[\"text_features\"] for item in response])\n",
    "\n",
    "    def _calculate_probability(\n",
    "        self, image_features: np.ndarray, text_features: np.ndarray, texts: List[str]\n",
    "    ):\n",
    "        \"\"\"Calculate probability scores between image and text embeddings.\"\"\"\n",
    "\n",
    "        logits_per_image = (\n",
    "            torch.from_numpy(self.scaling_factor).exp()\n",
    "            * torch.from_numpy(image_features)\n",
    "            @ torch.from_numpy(text_features).t()\n",
    "        )\n",
    "        probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "        return {text: float(prob) for text, prob in zip(texts, probs)}\n",
    "\n",
    "    @kernel_function()\n",
    "    def classify_image(\n",
    "        self,\n",
    "        labels: Annotated[list[str], \"List of category labels\"],\n",
    "        image_id: Annotated[str, \"Image ID to classify\"],\n",
    "    ) -> Annotated[dict, \"Categories mapped to probability scores\"]:\n",
    "        \"\"\"Classifies an image against a list of categories and returns probabilities as a dictionary with the input labels as keys.\"\"\"\n",
    "\n",
    "        # Get image from data access\n",
    "        image_data = self.data_access.get_image(image_id)\n",
    "\n",
    "        if image_data is None:\n",
    "            return {\"error\": f\"Image {image_id} not found\"}\n",
    "\n",
    "        print_message(\n",
    "            \"Tool (classify_image)\",\n",
    "            f\"Classifying image {image_id} ({len(image_data)} bytes).\\nLabels: {labels}\",\n",
    "        )\n",
    "\n",
    "        image_features = self._get_image_embeddings(image_data)\n",
    "        text_features = self._get_text_embeddings(labels)\n",
    "\n",
    "        # Calculate probabilities\n",
    "        probabilities = self._calculate_probability(\n",
    "            image_features, text_features, labels\n",
    "        )\n",
    "\n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527719ba",
   "metadata": {},
   "source": [
    "## 6. Create and Configure the Agent\n",
    "\n",
    "Now we bring all the components together to create our medical image classification agent.\n",
    "\n",
    "### Agent Instructions\n",
    "\n",
    "The agent is configured with detailed instructions that define its behavior:\n",
    "- How to extract and format classification labels from user queries\n",
    "- When and how to call the classification plugin\n",
    "- How to present results to users\n",
    "- Example interactions to guide the agent's responses\n",
    "\n",
    "#### Label Format\n",
    "\n",
    "For optimal results, text labels should follow this hierarchical structure:\n",
    "```\n",
    "<modality> <body part> <view information> <pathology/condition>\n",
    "```\n",
    "\n",
    "Examples:\n",
    "- \"x-ray chest anteroposterior pneumonia\"\n",
    "- \"computed tomography chest axial mass\"\n",
    "- \"magnetic resonance imaging brain sagittal tumor\"\n",
    "\n",
    "### Initialization Steps\n",
    "\n",
    "1. Create the chat context\n",
    "2. Instantiate the classification plugin with the context\n",
    "3. Register the plugin with the kernel\n",
    "4. Create the agent with the kernel and instructions\n",
    "5. Link the agent back to the context\n",
    "\n",
    "This design allows for clean separation of concerns while maintaining necessary connections between components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8113bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = ChatContext()\n",
    "\n",
    "plugin = ImageClassificationPlugin(context)\n",
    "# Add plugin to kernel with context's data access\n",
    "kernel.add_plugin(plugin, plugin_name=\"ImageClassifier\")\n",
    "\n",
    "# Create agent\n",
    "agent = ChatCompletionAgent(\n",
    "    kernel=kernel,\n",
    "    name=\"ImageClassificationAgent\",\n",
    "    instructions=\"\"\"\n",
    "You are a medical image classification assistant that helps classify medical images.\n",
    "\n",
    "When a user provides a message with an Image ID:\n",
    "1. Extract the categories from their message as a list of strings\n",
    "2. Each category should follow increasing specificity when possible: <modality> <body part> <view information> <pathology/condition>\n",
    "3. If any component is not available or not specified, leave it out\n",
    "4. Call classify_image with the labels and the image_id\n",
    "5. Report the probabilities to the user in a clear format\n",
    "\n",
    "Label Format Examples:\n",
    "- \"x-ray chest anteroposterior atelectasis\"\n",
    "- \"x-ray chest anteroposterior pneumonia\"\n",
    "- \"computed tomography chest axial mass\"\n",
    "- \"magnetic resonance imaging knee sagittal torn meniscus\"\n",
    "- \"histopathology H&E stain sentinel lymph node malignant\"\n",
    "- \"retinal fundus pathological myopia\"\n",
    "- \"dermatology clinical photography angular cheilitis\"\n",
    "- \"x-ray chest pneumonia\" (when view information not specified)\n",
    "- \"chest x-ray\" (when only modality and body part known)\n",
    "\n",
    "If the user provides abbreviated or informal labels (e.g., \"CT chest with mass\", \"MRI brain tumor\"), expand them.\n",
    "If the user asks general questions, help them formulate appropriate medical imaging labels.\n",
    "\n",
    "Example:\n",
    "User: \"is this atelectasis or pneumonia?\\nImage ID: abc123\"\n",
    "You: Call classify_image(labels=[\"x-ray chest anteroposterior atelectasis\", \"x-ray chest anteroposterior pneumonia\"], image_id=\"abc123\")\n",
    "\"\"\".strip(),\n",
    ")\n",
    "\n",
    "# Set the agent in the context\n",
    "context.set_agent(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b64767c",
   "metadata": {},
   "source": [
    "## 7. Example: Classify a Medical Image\n",
    "\n",
    "Now let's test our agent with a real medical image. In this example, we'll:\n",
    "1. Load a chest X-ray image\n",
    "2. Display it for visual reference\n",
    "3. Ask the agent to classify it among several possible conditions\n",
    "\n",
    "The agent will:\n",
    "- Understand the natural language query\n",
    "- Extract the relevant classification labels\n",
    "- Call the classification plugin\n",
    "- Return probability scores for each condition\n",
    "\n",
    "### Try Your Own Images\n",
    "\n",
    "You can easily adapt this example to classify your own medical images by:\n",
    "- Changing the `input_image` path\n",
    "- Modifying the question to include relevant conditions\n",
    "- Using different medical imaging modalities (CT, MRI, etc.)\n",
    "\n",
    "### Understanding the Results\n",
    "\n",
    "The agent will return probability scores for each label. Higher probabilities indicate stronger similarity between the image and that particular condition description in the MedImageInsight embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618e7f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "input_image = os.path.join(settings.DATA_ROOT, \"segmentation-examples/covid_1585.png\")\n",
    "\n",
    "# Send message with image\n",
    "await context.send_message(\n",
    "    message=\"Is this a chest x-ray showing COVID-19, pneumonia, or atelectasis?\",\n",
    "    image=input_image,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b9b93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = os.path.join(\n",
    "    settings.DATA_ROOT,\n",
    "    \"medimageinsight-outlier-detection/samples/test/outlier/CT/1.3.6.1.4.1.55648.010293352392778028677215985701318018213/1.3.6.1.4.1.55648.010293352392778028677215985701318018213.3.png\",\n",
    ")\n",
    "message = (\n",
    "    \"Could you tell me what modality this image is (CT, MRI, XRAY, mammography)\\n\"\n",
    "    + \"and what body part (extremity, chest, abdomen, brain, spine, pelvis, breast)?\"\n",
    ")\n",
    "\n",
    "# Send message with image\n",
    "await context.send_message(message=message, image=input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b396eec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = os.path.join(\n",
    "    settings.DATA_ROOT,\n",
    "    \"medimageinsight-outlier-detection/samples/test/outlier/MR/1.3.6.1.4.1.55648.002676776301544845833524448635393145729/1.3.6.1.4.1.55648.002676776301544845833524448635393145729.502.png\",\n",
    ")\n",
    "\n",
    "# Send message with image\n",
    "await context.send_message(message=message, image=input_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ce5a3a",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This tutorial demonstrated how to build an AI agent that combines natural language understanding with medical image classification using MedImageInsight embeddings. By leveraging Semantic Kernel, we created an intuitive conversational interface that allows users to ask questions about medical images in plain language without needing to understand the underlying technical implementation.\n",
    "\n",
    "The key advantage of this approach is flexibility—the agent can adapt to different classification tasks through natural conversation, making it valuable for:\n",
    "- **Rapid prototyping** of classification workflows without writing new code\n",
    "- **Clinical decision support** where physicians can ask specific diagnostic questions\n",
    "- **Research exploration** to quickly test hypotheses across different pathology categories\n",
    "- **Educational applications** where students can learn through interactive questioning\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "### Creating Different Types of Agents\n",
    "\n",
    "Now that you understand the fundamentals, you can create different types of specialized agents by combining concepts from other notebooks:\n",
    "\n",
    "**Adapter-Enhanced Classification Agents** - Use the [adapter training approach](./adapter-training.ipynb) to build agents with fine-tuned classification layers on top of embeddings. This can significantly boost accuracy for specific tasks like pneumonia detection or tumor classification while maintaining the conversational interface.\n",
    "\n",
    "**Segmentation + Classification Agents** - Integrate [MedImageParse](../medimageparse/medimageparse_segmentation_demo.ipynb) to create agents that provide both classification and visual localization. These agents could answer questions like \"Where is the lesion?\" by highlighting affected regions alongside classification probabilities.\n",
    "\n",
    "**Quality Control Agents** - Incorporate the [outlier detection methods](./outlier-detection-demo.ipynb) to create agents that flag unusual cases, detect acquisition problems, or identify studies that deviate from expected protocols—essential for maintaining data quality in clinical and research settings.\n",
    "\n",
    "**Multi-Modal Diagnostic Agents** - Combine multiple data types using patterns from the [advanced demos](../advanced_demos/). For example, build agents that analyze radiology images, pathology slides, and clinical notes together for comprehensive diagnostic support in complex cases like cancer staging.\n",
    "\n",
    "### Multi-Agent Orchestration with Healthcare Agent Orchestrator\n",
    "\n",
    "For production scenarios requiring coordination between multiple specialized agents, explore the [**Healthcare Agent Orchestrator**](https://github.com/Azure-Samples/healthcare-agent-orchestrator/) (HAO) project. HAO is a multi-agent framework specifically designed for complex healthcare workflows where different agents need to collaborate and share context.\n",
    "\n",
    "HAO demonstrates how to:\n",
    "- **Coordinate multiple specialized agents** working together on complex tasks like cancer care coordination\n",
    "- **Integrate with Microsoft Teams** for real-time collaboration between AI agents and care teams\n",
    "- **Work across diverse data types** including imaging, pathology, clinical notes, and structured data\n",
    "- **Build modular, scalable solutions** where agents can be added or modified without disrupting the overall system\n",
    "- **Connect with enterprise systems** like Copilot Studio through Microsoft Cloud for Healthcare\n",
    "\n",
    "This is particularly valuable for multi-disciplinary scenarios where a classification agent might work alongside report generation agents, scheduling agents, and care coordination agents—all collaborating to support the clinical team.\n",
    "\n",
    "### Related Notebooks\n",
    "\n",
    "- [Zero-Shot Classification](./zero-shot-classification.ipynb) - Deep dive into the embedding-based classification approach\n",
    "- [Adapter Training](./adapter-training.ipynb) - Fine-tune for improved accuracy on specific tasks\n",
    "- [Outlier Detection](./outlier-detection-demo.ipynb) - Identify unusual cases automatically\n",
    "- [Advanced Demos](../advanced_demos/) - Multi-modal and complex healthcare applications"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "haitk-py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
